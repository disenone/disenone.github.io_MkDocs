---
layout: post
title: Documento de instrucciones del complemento UE AIChatPlus
tags:
- dev
- game
- UE
- UnreanEngine
- UE4
- UE5
- Editor
- Editor Plus
- Editor Plugin
- AI Chat
- Chatbot
- Image Generation
- OpenAI
- Azure
- Claude
- Gemini
- Ollama
description: Manual de instrucciones del complemento UE AIChatPlus
---

<meta property="og:title" content="UE Êèí‰ª∂ AIChatPlus ËØ¥ÊòéÊñáÊ°£" />

#Documentaci√≥n de UE AIChatPlus Plugin

##Dep√≥sito p√∫blico

[UE.AIChatPlus.Public](https://github.com/disenone/UE.AIChatPlus.Public)

##Obtener complemento

[AIChatPlus](https://www.unrealengine.com/marketplace/zh-CN/product/aichatplus-ai-chat-integration-openai-azure-claude-gemini)

##Introducci√≥n del complemento

Este complemento es compatible con UE5.2+.

UE.AIChatPlus es un complemento de UnrealEngine que implementa la comunicaci√≥n con varios servicios de chat de IA GPT. Actualmente admite servicios como OpenAI (ChatGPT, DALL-E), Azure OpenAI (ChatGPT, DALL-E), Claude, Google Gemini, Ollama y llama.cpp en modo offline local. En el futuro seguir√° agregando soporte para m√°s proveedores de servicios. Su implementaci√≥n se basa en solicitudes REST as√≠ncronas, lo que lo hace eficiente y conveniente para que los desarrolladores de Unreal Engine se integren con estos servicios de chat de IA.

UE.AIChatPlus tambi√©n incluye una herramienta de edici√≥n que permite utilizar directamente los servicios de chat de AI en el editor para generar texto e im√°genes, y analizar im√°genes, entre otras funciones.

##Instrucciones de uso

###Herramienta de chat del editor.

El men√∫ Herramientas -> AIChatPlus -> AIChat abre la herramienta de chat del editor proporcionada por el complemento.

![](assets/img/2024-ue-aichatplus/chat_tool3.png)


El software ofrece funciones de generaci√≥n de texto, chat de texto, generaci√≥n de im√°genes y an√°lisis de im√°genes.

La interfaz de la herramienta es aproximadamente:

![text chat](assets/img/2024-ue-aichatplus/chat_tool2.png)

![image chat](assets/img/2024-ue-aichatplus/chat_tool.png)

####Funci√≥n principal

* Modelo grande sin conexi√≥n: Integraci√≥n de la biblioteca llama.cpp para admitir la ejecuci√≥n sin conexi√≥n de modelos grandes a nivel local

* Chat de texto: haz clic en el bot√≥n `Nuevo Chat` en la esquina inferior izquierda para crear una nueva sesi√≥n de chat de texto.

* Generaci√≥n de im√°genes: haz clic en el bot√≥n `New Image Chat` en la esquina inferior izquierda para crear una nueva sesi√≥n de generaci√≥n de im√°genes.

An√°lisis de im√°genes: Algunos servicios de chat en la funci√≥n 'New Chat' admiten el env√≠o de im√°genes, como Claude, Google Gemini. Simplemente haz clic en los botones üñºÔ∏è o üé® encima del cuadro de entrada para cargar la imagen que deseas enviar.

* Soporte de Blueprint: permite la creaci√≥n de solicitudes de API con Blueprint para funciones como chat de texto, generaci√≥n de im√°genes, entre otras.

Establecer el rol actual de la conversaci√≥n: El men√∫ desplegable en la parte superior del cuadro de chat permite seleccionar el rol actual para enviar mensajes, lo que permite simular diferentes roles para ajustar la conversaci√≥n con IA.

Eliminar chat: Pulsar la ‚úñÔ∏è en la parte superior de la ventana del chat permite borrar el historial de mensajes de la conversaci√≥n actual.

* Plantilla de di√°logo: incorpora cientos de configuraciones de di√°logo para facilitar el manejo de problemas comunes.

* Configuraci√≥n global: al hacer clic en el bot√≥n `Setting` en la esquina inferior izquierda, se abrir√° la ventana de configuraci√≥n global. Puede configurar el chat de texto predeterminado, los servicios de API de generaci√≥n de im√°genes y establecer los par√°metros espec√≠ficos de cada servicio de API. La configuraci√≥n se guardar√° autom√°ticamente en la ruta del proyecto `$(ProjectFolder)/Saved/AIChatPlusEditor`.

Translate these text into Spanish language:

* **Configuraci√≥n de la conversaci√≥n:** Haga clic en el bot√≥n de configuraci√≥n en la parte superior del cuadro de chat para abrir la ventana de configuraci√≥n de la conversaci√≥n actual. Permite modificar el nombre de la conversaci√≥n, cambiar el servicio API utilizado en la conversaci√≥n y configurar par√°metros espec√≠ficos del API para cada conversaci√≥n por separado. La configuraci√≥n de la conversaci√≥n se guarda autom√°ticamente en `$(ProjectFolder)/Saved/AIChatPlusEditor/Sessions`

Modificar el contenido del chat: al pasar el rat√≥n por encima del contenido del chat, aparecer√° un bot√≥n de configuraci√≥n para ese contenido espec√≠fico, que permitir√° regenerar, modificar, copiar o eliminar el contenido, as√≠ como regenerar el contenido debajo (en el caso de que el personaje sea un usuario).

* Visualizaci√≥n de im√°genes: para la generaci√≥n de im√°genes, al hacer clic en una imagen se abrir√° una ventana de visualizaci√≥n de im√°genes (ImageViewer), que permite guardar la imagen como PNG/UE Texture. Las Textures se pueden ver directamente en el Explorador de contenido (Content Browser), facilitando su uso en el editor. Tambi√©n es posible eliminar im√°genes, regenerarlas, continuar generando m√°s im√°genes, entre otras funciones. Para los editores en Windows, tambi√©n se puede copiar im√°genes para as√≠ tenerlas disponibles en el portapapeles, lo que facilita su uso. Las im√°genes generadas durante una sesi√≥n tambi√©n se guardar√°n autom√°ticamente en la carpeta de cada sesi√≥n, generalmente en la ruta `$(ProjectFolder)/Saved/AIChatPlusEditor/Sessions/${GUID}/images`.

Blueprint:

![blueprint](assets/img/2024-ue-aichatplus/blueprint.png)

Configuraci√≥n general:

![global settings](assets/img/2024-ue-aichatplus/global_setting.png)

Configuraci√≥n de la conversaci√≥n:

![session settings](assets/img/2024-ue-aichatplus/session_setting.png)

Modificar el contenido del chat:

![chat edit](assets/img/2024-ue-aichatplus/chat_edit.png)

Visor de im√°genes:

![image viewer](assets/img/2024-ue-aichatplus/image_viewer.png)

Utilizaci√≥n de modelos grandes sin conexi√≥n

![offline model](assets/img/2024-ue-aichatplus/offline_model.png)

Plantilla de di√°logo

![system template](assets/img/2024-ue-aichatplus/system_template.png)

###Presentaci√≥n del c√≥digo principal

En la actualidad, el complemento se divide en los siguientes m√≥dulos:

* AIChatPlusCommon: M√≥dulo de tiempo de ejecuci√≥n (Runtime), encargado de manejar solicitudes de env√≠o de diversas interfaces de API de IA y analizar el contenido de las respuestas.

* AIChatPlusEditor: M√≥dulo de editor, encargado de implementar la herramienta de chat AI del editor.

* AIChatPlusCllama: M√≥dulo de tiempo de ejecuci√≥n (Runtime), responsable de encapsular la interfaz y los par√°metros de llama.cpp, para llevar a cabo la ejecuci√≥n de modelos grandes sin conexi√≥n.

* Thirdparty/LLAMACpp: M√≥dulo de terceros en tiempo de ejecuci√≥n (Runtime) que integra la biblioteca din√°mica y archivos de cabecera de llama.cpp.

Traduce estos textos al idioma espa√±ol:

El UClass espec√≠fico responsable de enviar la solicitud es FAIChatPlus_xxxChatRequest, cada tipo de servicio API tiene su propio UClass de solicitud independiente. Las respuestas de las solicitudes se obtienen a trav√©s de dos tipos de UClass, UAIChatPlus_ChatHandlerBase / UAIChatPlus_ImageHandlerBase, solo necesitas registrar el delegado de devoluci√≥n de llamada correspondiente.

Antes de enviar la solicitud, es necesario configurar los par√°metros de la API y el mensaje a enviar. Esto se hace mediante FAIChatPlus_xxxChatRequestBody. El contenido espec√≠fico de la respuesta tambi√©n se analiza en FAIChatPlus_xxxChatResponseBody, y al recibir la llamada de vuelta, se puede obtener el ResponseBody a trav√©s de una interfaz espec√≠fica.

M√°s detalles del c√≥digo fuente est√°n disponibles en la tienda de UE: [AIChatPlus](https://www.unrealengine.com/marketplace/zh-CN/product/aichatplus-ai-chat-integration-openai-azure-claude-gemini)

##Gu√≠a de uso

###Utiliza el modelo fuera de l√≠nea del editor de herramientas llama.cpp.

Traduce este texto al idioma espa√±ol:

**Instrucciones sobre c√≥mo utilizar el modelo offline llama.cpp en la herramienta de edici√≥n AIChatPlus**

Descargue el modelo sin conexi√≥n desde el sitio web de HuggingFace: [Qwen1.5-1.8B-Chat-Q8_0.gguf](https://huggingface.co/second-state/Qwen1.5-1.8B-Chat-GGUF/resolve/main/Qwen1.5-1.8B-Chat-Q8_0.gguf)

Coloca el modelo en una carpeta espec√≠fica, por ejemplo, dentro del directorio del proyecto de juego Content/LLAMA.

```shell
E:/UE/projects/FP_Test1/Content/LLAMA
> ls
qwen1.5-1_8b-chat-q8_0.gguf*
```

Abre la herramienta de edici√≥n AIChatPlus: Herramientas -> AIChatPlus -> AIChat, crea una nueva sesi√≥n de chat y abre la p√°gina de configuraci√≥n de la sesi√≥n.

![guide editor](assets/img/2024-ue-aichatplus/guide_editor_1.png)

Establecer Api como Cllama, activar Configuraci√≥n Personalizada de Api y agregar la ruta de b√∫squeda de modelos, luego seleccionar un modelo.

![guide editor](assets/img/2024-ue-aichatplus/guide_editor_2.png)

Comience a chatear!!

![guide editor](assets/img/2024-ue-aichatplus/guide_editor_3.png)

###El c√≥digo utiliza el modelo fuera de l√≠nea llama.cpp

Traduce este texto al espa√±ol:

El siguiente texto explica c√≥mo usar el modelo fuera de l√≠nea llama.cpp en el c√≥digo.

Primero, tambi√©n necesitas descargar el archivo del modelo en Content/LLAMA.

A√±ade una l√≠nea de c√≥digo para incluir un comando y enviar un mensaje al modelo fuera de l√≠nea desde dicho comando.

```c++
#include "Common/AIChatPlus_Log.h"
#include "Common_Cllama/AIChatPlus_CllamaChatRequest.h"

void AddTestCommand()
{
	IConsoleManager::Get().RegisterConsoleCommand(
		TEXT("AIChatPlus.TestChat"),
		TEXT("Test Chat."),
		FConsoleCommandDelegate::CreateLambda([]()
		{
			if (!FModuleManager::GetModulePtr<FAIChatPlusCommon>(TEXT("AIChatPlusCommon"))) return;

			TWeakObjectPtr<UAIChatPlus_ChatHandlerBase> HandlerObject = UAIChatPlus_ChatHandlerBase::New();
			// Cllama
			FAIChatPlus_CllamaChatRequestOptions Options;
			Options.ModelPath.FilePath = FPaths::ProjectContentDir() / "LLAMA" / "qwen1.5-1_8b-chat-q8_0.gguf";
			Options.NumPredict = 400;
			Options.bStream = true;
			// Options.StopSequences.Emplace(TEXT("json"));
			auto RequestPtr = UAIChatPlus_CllamaChatRequest::CreateWithOptionsAndMessages(
				Options,
				{
					{"You are a chat bot", EAIChatPlus_ChatRole::System},
					{"who are you", EAIChatPlus_ChatRole::User}
				});

			HandlerObject->BindChatRequest(RequestPtr);
			const FName ApiName = TEnumTraits<EAIChatPlus_ChatApiProvider>::ToName(RequestPtr->GetApiProvider());

			HandlerObject->OnMessage.AddLambda([ApiName](const FString& Message)
			{
				UE_LOG(AIChatPlus_Internal, Display, TEXT("TestChat[%s] Message: [%s]"), *ApiName.ToString(), *Message);
			});
			HandlerObject->OnStarted.AddLambda([ApiName]()
			{
				UE_LOG(AIChatPlus_Internal, Display, TEXT("TestChat[%s] RequestStarted"), *ApiName.ToString());
			});
			HandlerObject->OnFailed.AddLambda([ApiName](const FAIChatPlus_ResponseErrorBase& InError)
			{
				UE_LOG(AIChatPlus_Internal, Error, TEXT("TestChat[%s] RequestFailed: %s "), *ApiName.ToString(), *InError.GetDescription());
			});
			HandlerObject->OnUpdated.AddLambda([ApiName](const FAIChatPlus_ResponseBodyBase& ResponseBody)
			{
				UE_LOG(AIChatPlus_Internal, Display, TEXT("TestChat[%s] RequestUpdated"), *ApiName.ToString());
			});
			HandlerObject->OnFinished.AddLambda([ApiName](const FAIChatPlus_ResponseBodyBase& ResponseBody)
			{
				UE_LOG(AIChatPlus_Internal, Display, TEXT("TestChat[%s] RequestFinished"), *ApiName.ToString());
			});

			RequestPtr->SendRequest();
		}),
		ECVF_Default
	);
}
```

* Despu√©s de volver a compilar, puedes utilizar comandos en la terminal del editor para ver los resultados de la salida del modelo grande en el registro OutputLog.

![guide code](assets/img/2024-ue-aichatplus/guide_code_1.png)

###La utilizaci√≥n de un modelo fuera de l√≠nea en el archivo llama.cpp.

La siguiente descripci√≥n explica c√≥mo utilizar el modelo offline llama.cpp en un blueprint.

Crea un nodo `Send Cllama Chat Request` haciendo clic derecho en el diagrama.

![guide bludprint](assets/img/2024-ue-aichatplus/guide_blueprint_1.png)

Cree el nodo Options y configure `Stream=true, ModelPath="E:\UE\projects\FP_Test1\Content\LLAMA\qwen1.5-1_8b-chat-q8_0.gguf"`

![guide bludprint](assets/img/2024-ue-aichatplus/guide_blueprint_2.png)

![guide bludprint](assets/img/2024-ue-aichatplus/guide_blueprint_3.png)

* Crea Mensajes, a√±ade un Mensaje de Sistema y un Mensaje de Usuario respectivamente

![guide bludprint](assets/img/2024-ue-aichatplus/guide_blueprint_4.png)

* Establecer un delegado que reciba la informaci√≥n de salida del modelo y la imprima en la pantalla

![guide bludprint](assets/img/2024-ue-aichatplus/guide_blueprint_5.png)

![guide bludprint](assets/img/2024-ue-aichatplus/guide_blueprint_6.png)

Translate these text into Spanish language: 

* El aspecto de un diagrama completo es as√≠, al ejecutarlo, ver√° en la pantalla del juego el mensaje devuelto al imprimir el gran modelo.

![guide bludprint](assets/img/2024-ue-aichatplus/guide_blueprint_7.png)

![guide bludprint](assets/img/2024-ue-aichatplus/guide_blueprint_8.png)

##Registro de actualizaciones

### v1.3.1 - 2024.9.30

Agregar un SystemTemplateViewer que permita ver y utilizar cientos de plantillas de configuraci√≥n del sistema.

#### Bugfix

Repare el complemento descargado desde la tienda, no se encuentra la biblioteca de enlace llamada llama.cpp
Corregir problema de ruta demasiado larga en LLAMACpp
Reparar el error llama.dll despu√©s de empaquetar en Windows.
* Resolver problema de lectura de ruta de archivo en ios/android
Corregir el error de configuraci√≥n del nombre de Cllame.

### v1.3.0 - 2024.9.23

Actualizaci√≥n importante

* Se ha integrado llama.cpp, lo que permite la ejecuci√≥n offline de grandes modelos localmente

### v1.2.0 - 2024.08.20

Apoyo a OpenAI Image Edit/Image Variation.

Apoyar la API de Ollama, apoyar la obtenci√≥n autom√°tica de la lista de modelos admitidos por Ollama

### v1.1.0 - 2024.08.07

Apoyo al plan.

### v1.0.0 - 2024.08.05

* Funcionalidad completa y b√°sica

Apoyo a OpenAI, Azure, Claude, Gemini.

* Herramienta de chat con un editor incorporado completa

--8<-- "footer_en.md"


> Este post est√° traducido usando ChatGPT, por favor [**feedback**](https://github.com/disenone/wiki_blog/issues/new) si hay alguna omisi√≥n.
